{"cells":[{"cell_type":"code","execution_count":null,"id":"3b73b213-58af-4209-9efd-ac34c9e1e1d7","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# ADD COMMENTS NOTES "]},{"cell_type":"code","execution_count":null,"id":"33be8396-a753-45f0-9f8a-fd4ea2177d88","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["import pandas as pd\n","from pyspark.sql.functions import col, date_sub, expr, to_date, date_add, explode, split\n","import random\n","import uuid"]},{"cell_type":"code","execution_count":null,"id":"0da9e0a4-d922-45bd-8721-c519eb22bb68","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# DEBUG:\n","# df = spark.sql(\"SELECT * FROM ckm_conv_processed \")\n","# display(df)"]},{"cell_type":"code","execution_count":null,"id":"9bf126cc-c972-4460-8eee-2c9440203fcc","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["df = spark.sql(\"SELECT * FROM ckm_conv_processed\")\n","# display(df)"]},{"cell_type":"code","execution_count":null,"id":"47223675-fc8a-4f65-8f8b-9895117e02c4","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# generate keyphrases table\n","\n","df_keyphrases = df.withColumn(\"keyPhrases\", explode(split(col(\"keyPhrases\"), \",\\s\")))\n","\n","df_keyphrases = df_keyphrases.select(\"ConversationId\", \"KeyPhrases\")\n","\n","df_keyphrases = df_keyphrases.withColumnRenamed(\"KeyPhrase\", \"Keyphrase\")\n","\n","\n","df_keyphrases.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('ckm_conv_processed_keyphrases')\n","# df_keyphrases.write.format('delta').mode('append')..saveAsTable('ckm_conv_processed_keyphrases')\n","\n","# df = spark.sql(\"SELECT * FROM ckm_lakehouse.ckm_conv_processed_keyphrases LIMIT 1000\")\n","# display(df)"]},{"cell_type":"code","execution_count":null,"id":"09a6a7fc-1701-41d4-a4e2-e9c0d5b6b6c4","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["record_count = df.count()\n","\n","# Print the number of records\n","print(f\"Total number of records in the DataFrame: {record_count}\")"]},{"cell_type":"code","execution_count":null,"id":"430c4303-0cc6-4afb-b375-e6da844ea90d","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["#### ADD COMMENTS TO MENTION THIS SHOULD ONLY FOR SAMPLE DATA\n","\n","\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, max as spark_max, current_timestamp, unix_timestamp, from_unixtime, expr,lit\n","from pyspark.sql.types import TimestampType\n","\n","# Convert string columns to timestamp types\n","df = df.withColumn(\"StartTime\", col(\"StartTime\").cast(\"timestamp\"))\n","df = df.withColumn(\"EndTime\", col(\"EndTime\").cast(\"timestamp\"))\n","df = df.withColumn(\"ConversationDate\", col(\"ConversationDate\").cast(\"timestamp\"))\n","\n","# Calculate the maximum StartTime\n","max_date_df = df.select(spark_max(col(\"StartTime\")).alias(\"max_date\"))\n","max_date = max_date_df.collect()[0][\"max_date\"]\n","\n","# Get current timestamp\n","current_ts = spark.sql(\"SELECT current_timestamp() as current_ts\").collect()[0][\"current_ts\"]\n","\n","print(\"max_date: \", max_date)\n","print(\"current time: \", current_ts)\n","\n","# Calculate the difference in seconds between the current timestamp and the maximum StartTime\n","time_diff_seconds = (current_ts - max_date).total_seconds()\n","\n","# Convert the time difference to days, hours, minutes, and seconds\n","days = int(time_diff_seconds // (24 * 3600))\n","hours = int((time_diff_seconds % (24 * 3600)) // 3600)\n","minutes = int((time_diff_seconds % 3600) // 60)\n","seconds = int(time_diff_seconds % 60)\n","\n","# Total number of records\n","total_records = df.count()\n","\n","# Calculate the number of records for each time range\n","today_count = int(total_records) * .4\n","yesterday_today_count = int(total_records * 0.25)\n","two_days_prior_count = int(total_records * 0.1)\n","last_7_days_count = int(total_records * 0.15)\n","current_month_count = int(total_records * 0.1)\n","prior_month_count = total_records - (yesterday_today_count + two_days_prior_count + current_month_count)\n","\n","# # Assign random dates based on the calculated counts\n","# df_temp = df.withColumn(\"row_num\", expr(\n","#         f\"\"\"\n","#         CASE\n","#             WHEN rand() <= {today_count / record_count} THEN 1\n","#             WHEN rand() <= {yesterday_today_count / total_records} THEN 1\n","#             WHEN rand() <= {(yesterday_today_count + two_days_prior_count) / total_records} THEN 2\n","#             WHEN rand() <= {(last_7_days_count + yesterday_today_count + two_days_prior_count ) / total_records} THEN 3\n","#             WHEN rand() <= {(yesterday_today_count + two_days_prior_count + current_month_count) / total_records} THEN 4\n","#             ELSE 5\n","#         END\n","#         \"\"\"\n","#     ))\n","\n","# # Generate new dates based on row_num\n","# df_temp = df_temp.withColumn(\"NewStartTime\", expr(\n","#      f\"\"\"\n","#         CASE\n","#             WHEN row_num = 1 THEN current_date()\n","#             WHEN row_num = 2 THEN date_add(current_date(), -1)\n","#             WHEN row_num = 3 THEN date_add(current_date(), -7)\n","#             WHEN row_num = 4 THEN date_add(trunc(current_date(), 'MM'), cast(rand() * day(current_date()) as int))\n","#             ELSE date_add(current_date(), -30 + cast(rand() * 30 as int))\n","#         END\n","#         \"\"\"\n","#     ).cast('timestamp'))\n","\n","# WHEN rand() < {(last_7_days_count + two_days_prior_count + yesterday_today_count + today_count) / total_records} THEN 4\n","# WHEN row_num = 4 THEN date_add(trunc(current_date(), 'MM'), cast(rand() * day(current_date()) as int))\n","\n","df_temp = df.withColumn(\"row_num\", expr(\n","    f\"\"\"\n","    CASE\n","        WHEN rand() < {today_count / total_records} THEN 1\n","        WHEN rand() < {(yesterday_today_count + today_count) / total_records} THEN 2\n","        WHEN rand() < {(last_7_days_count + yesterday_today_count + today_count) / total_records} THEN 3\n","        ELSE 4\n","    END\n","    \"\"\"\n","))\n","\n","# Generate new dates based on row_num\n","df_temp = df_temp.withColumn(\"NewStartTime\", expr(\n","    \"\"\"\n","    CASE\n","        WHEN row_num = 1 THEN current_date()\n","        WHEN row_num = 2 THEN date_add(current_date(), -1)\n","        WHEN row_num = 3 THEN date_add(current_date(), -cast(rand() * 7 as int))\n","        ELSE date_add(date_add(current_date(), -7), -30 + cast(rand() * 30 as int))\n","    END\n","    \"\"\"\n",").cast('timestamp'))\n","\n","\n","# Combine the new date with the original time part of StartTime\n","df_temp = df_temp.withColumn(\"StartTime\", expr(\"to_timestamp(concat(date_format(NewStartTime, 'yyyy-MM-dd'), ' ', date_format(StartTime, 'HH:mm:ss.SSS')))\"))\n","\n","\n","# Adjust EndTime based on NewStartTime and Duration (Duration is in minutes)\n","interval_str = \"Duration minutes\"\n","df_temp = df_temp.withColumn(\"EndTime\", expr(\"StartTime + make_interval(0, 0, 0, 0, 0, Duration, 0)\"))\n","\n","\n","# Print the time difference in a sentence\n","# print(f\"The difference between the current time and the maximum date is {days} days, {hours} hours, {minutes} minutes, and {seconds} seconds.\")\n","\n","\n","\n","# Combine the new date with the original time part of ConversationDate to form NewConversationDate\n","df_temp = df_temp.withColumn(\"ConversationDate\", expr(\"concat(date_format(StartTime, 'yyyy-MM-dd'), ' ', date_format(ConversationDate, 'HH:mm:ss.SSS'))\"))\n","df_temp = df_temp.withColumn(\"ConversationDate\", col(\"ConversationDate\").cast(\"timestamp\"))\n","\n","\n","# Drop helper columns\n","df_temp = df_temp.drop(\"row_num\", \"NewStartTime\")\n","\n","# display(df_temp)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"21226171-51c8-4c48-8f3f-97177127f3da","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# display(df_temp)"]},{"cell_type":"code","execution_count":null,"id":"625a9b18-b7fc-44ba-99e4-6dce6536aa72","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# #### ADD COMMENTS TO MENTION THIS SHOULD ONLY FOR SAMPLE DATA\n","\n","# from pyspark.sql import SparkSession\n","# from pyspark.sql.functions import col, expr, date_sub, current_date, date_format, concat\n","# from pyspark.sql.types import TimestampType\n","# import random\n","# from datetime import datetime, timedelta\n","\n","# # # Initialize Spark session\n","# # spark = SparkSession.builder.appName(\"sample_data\").getOrCreate()\n","\n","# # # Print the current date using datetime module\n","# current_date = datetime.now().date()\n","# # print(\"Current date using datetime module:\", current_date_python)\n","\n","# # # Print the current date using PySpark\n","# # df_date = spark.sql(\"SELECT current_date() as current_date\")\n","# # df_date.show()\n","\n","# # Convert string columns to timestamp types\n","# df = df.withColumn(\"StartTime\", col(\"StartTime\").cast(\"timestamp\"))\n","# df = df.withColumn(\"EndTime\", col(\"EndTime\").cast(\"timestamp\"))\n","# df = df.withColumn(\"ConversationDate\", col(\"ConversationDate\").cast(\"timestamp\"))\n","\n","# # Total number of records\n","# total_records = df.count()\n","\n","# # Step 1: Ensure 1-3 records for each day in the last 30 days (excluding current and previous days)\n","# dates_last_30_days = [(datetime.now() - timedelta(days=i)).date() for i in range(2, 33)]\n","# num_records_per_day = {date: random.randint(1, 3) for date in dates_last_30_days}\n","# total_last_30_days_records = sum(num_records_per_day.values())\n","\n","\n","\n","# # Step 2: Calculate the remaining records and their distribution\n","# remaining_records = total_records - total_last_30_days_records\n","# current_date_count = int(remaining_records * 0.60)\n","# previous_date_count = remaining_records - current_date_count\n","\n","# # Generate the new StartTime values for the last 30 days\n","# dates_30_days = []\n","# for date, count in num_records_per_day.items():\n","#     dates_30_days.extend([date] * count)\n","# random.shuffle(dates_30_days)\n","\n","# # Generate the new StartTime values for current day and previosu day\n","# dates_remaining = [current_date] * current_date_count + [current_date - timedelta(days=1)] * previous_date_count\n","# random.shuffle(dates_remaining)\n","\n","# # Combine the two lists\n","# dates_final = dates_30_days + dates_remaining\n","\n","# # Create a DataFrame with a new StartTime distribution\n","# df_temp = df.withColumn(\"row_num\", expr(f\"row_number() over (order by rand())\"))\n","\n","# # Assign dates to rows based on row_num\n","# date_assignments = {i + 1: date for i, date in enumerate(dates_final)}\n","\n","# def get_date(row_num):\n","#     return date_assignments[row_num]\n","\n","# # Register the UDF\n","# from pyspark.sql.functions import udf\n","# from pyspark.sql.types import DateType\n","\n","# get_date_udf = udf(get_date, DateType())\n","\n","# # Assign NewStartTime\n","# df_temp = df_temp.withColumn(\"NewStartTime\", get_date_udf(col(\"row_num\")))\n","\n","# # Combine the new date with the original time part of StartTime\n","# df_temp = df_temp.withColumn(\"StartTime\", expr(\"to_timestamp(concat(date_format(NewStartTime, 'yyyy-MM-dd'), ' ', date_format(StartTime, 'HH:mm:ss.SSS')))\"))\n","\n","# # Adjust EndTime based on NewStartTime and Duration (Duration is in minutes)\n","# df_temp = df_temp.withColumn(\"EndTime\", expr(\"StartTime + make_interval(0, 0, 0, 0, 0, Duration, 0)\"))\n","\n","# # Combine the new date with the original time part of ConversationDate to form NewConversationDate\n","# df_temp = df_temp.withColumn(\"ConversationDate\", expr(\"concat(date_format(StartTime, 'yyyy-MM-dd'), ' ', date_format(ConversationDate, 'HH:mm:ss.SSS'))\"))\n","# df_temp = df_temp.withColumn(\"ConversationDate\", col(\"ConversationDate\").cast(\"timestamp\"))\n","\n","# # Drop helper columns\n","# df_temp = df_temp.drop(\"row_num\", \"NewStartTime\")\n","\n","# # Display the DataFrame\n","# # display(df_temp)\n"]},{"cell_type":"code","execution_count":null,"id":"f91bb363-38ca-4490-94f1-ec67b457aa0f","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["df_temp.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"true\").saveAsTable('ckm_conv_processed_temp')"]},{"cell_type":"code","execution_count":null,"id":"e8e036de-0d34-4ea5-ab75-b624ddc2e220","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["df = spark.sql(\"SELECT * FROM ckm_conv_processed_temp \")"]},{"cell_type":"code","execution_count":null,"id":"29752e89-e92b-4bc8-9a21-8aa2c6cd1979","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# display(df)"]},{"cell_type":"code","execution_count":null,"id":"82c35c12-b919-4e55-959a-2300f0412ee0","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["df.write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"false\").saveAsTable('ckm_conv_processed')"]},{"cell_type":"code","execution_count":null,"id":"d8304302-6439-41e5-b7ac-f48f1eb84fa5","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# df = spark.sql(\"SELECT * FROM ckm_conv_processed \")\n","# display(df)"]},{"cell_type":"code","execution_count":null,"id":"72a25ab0-2944-4504-8922-8f30aa5585b8","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["# df_view = spark.sql(\"select * from ckm_conv_processed\")\n","# display(df_view)"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"e6ad9dad-e3da-4da5-bca6-6572c466b69a","default_lakehouse_name":"ckm_lakehouse","default_lakehouse_workspace_id":"0d98d480-171b-4b4d-a8e7-80fbd031d1a6","known_lakehouses":[{"id":"e6ad9dad-e3da-4da5-bca6-6572c466b69a"}]}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
